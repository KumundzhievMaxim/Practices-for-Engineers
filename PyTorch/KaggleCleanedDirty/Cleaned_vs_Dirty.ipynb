{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!kaggle competitions download -c platesv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dirty = \n",
    "# cleand = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(os.getcwd() + '/plates.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_images_dir = '/Users/macbook/PycharmProjects/Python_Practices/PyTorch/KaggleCleanedDirty/plates/test'\n",
    "cleaned_train_images_dir = '/Users/macbook/PycharmProjects/Python_Practices/PyTorch/KaggleCleanedDirty/plates/train/cleaned'\n",
    "dirty_train_images_dir = '/Users/macbook/PycharmProjects/Python_Practices/PyTorch/KaggleCleanedDirty/plates/train/dirty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_images(directory):\n",
    "    os.chdir(directory)\n",
    "    for image in os.listdir(directory):\n",
    "        if image == '.DS_Store':\n",
    "            print('Apsent of file in directory?!?')\n",
    "            continue\n",
    "        display(Image.open('{}'.format(image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_images(dirty_train_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transorm data folders in convenient usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shutil.copytree?\n",
    "os.makedirs?\n",
    "os.chmod?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_val_folders():\n",
    "    root_dir = '/Users/macbook/PycharmProjects/Python_Practices/PyTorch/KaggleCleanedDirty'\n",
    "    os.chdir(root_dir)\n",
    "    \n",
    "    train_dir = 'train'\n",
    "    val_dir = 'val'\n",
    "\n",
    "    class_names = ['cleaned', 'dirty']\n",
    "\n",
    "    for dir_name in [train_dir, val_dir]:\n",
    "        for class_name in class_names:\n",
    "            os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
    "            \n",
    "    for class_name in class_names:\n",
    "        source_dir = os.path.join(root_dir,'plates', 'train', class_name)\n",
    "        for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n",
    "            if i % 6 != 0:\n",
    "                dest_dir = os.path.join(train_dir, class_name) \n",
    "            else:\n",
    "                dest_dir = os.path.join(val_dir, class_name)\n",
    "            shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))\n",
    "        \n",
    "            \n",
    "train_val_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augumentation of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "#torchvision.datasets.ImageFolder(allow us iterate throught images and get the tuples (image, image_id))\n",
    "import torchvision \n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, models\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\n",
    "val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formation of Train and Validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, transform())\n",
    "val_dataset = torchvision.datasets.ImageFolder(val_dir, augumentation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaned', 'dirty']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "train_dataset, batch_size = batch_size, shuffle=True, num_workers=batch_size)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "val_dataset, batch_size = batch_size, shuffle=True, num_workers=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader), len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # In order to have a look on the transformed images \n",
    "\n",
    "# X_batch, y_batch = next(iter(train_dataloader))\n",
    "# mean = np.array([0.485, 0.456, 0.406])\n",
    "# std = np.array([0.229, 0.224, 0.225])\n",
    "# plt.imshow(X_batch[0].permute(1, 2, 0).numpy() * std + mean);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_list = ['except', 'int', 'else', 'try', 'except', 'else', 'try', 'finally', 'finally', 'except', 'else', 'or', 'finally', 'for', 'for', 'yield', 'return', 'with', 'return', '@', 'reduce']\n",
    "\n",
    "# list_2 = ['.py, .pyw, .pyc, .pyo, .pyd<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup>', 'import this', 'else', 'try', 'except', 'logging', 'unittest', 'threading', 'finally', 'except', '@', 'int', 'long', 'int', 'complex', 'master', 'slave', 'parent', 'child', 'if', 'else', 'elif', 'while', 'for', 'break', 'continue', 'class', 'def', 'return', 'yield', 'try', 'except', 'else', 'try', 'finally', 'finally', 'except', 'else', 'pass', 'printf()', 'or', 'and', 'or', 's[N:M]', 'import keyword; print(keyword.kwlist)', '__doc__', \"is, '.', '='\", 'len()', 'import', 'reload()', 'try, except, else, finally, raise', 'else, except', 'finally', 'sys.exc_info()', 'with', 'for', 'itertools', 'next()', 'for', 'yield', 'return', 'send()', 'throw()', 'with', 'contextlib', 'return', '@', '@декоратор(аргументы)', 're', 'profile', 'timeit', 'sort()', '+=', '.pyc', '.pyo', 'int, str, float, list', 'io', '{k: v for k, v in a_dict}', '{el1, el2, el3}', 'print', 'from __future__ import print_function', 'reduce', 'map', 'filter', 'functools', 'reduce', 'exec', 'a, *rest, b = range(5)', 'def foo(a, (b, c))', 'pdb', 'help()']\n",
    "\n",
    "# def count_words(list):\n",
    "#     counter = 1\n",
    "#     dic = {}\n",
    "#     max_list = []\n",
    "#     for word in list:\n",
    "#         if word not in dic.keys():\n",
    "#             dic.update({'{}'.format(word) : counter})\n",
    "#         else:\n",
    "#             dic.update({'{}'.format(word) : dic[word] + 1})\n",
    "#     #print([key for key, value in dic.items() if value > 2])\n",
    "#     return dic\n",
    "    \n",
    "    \n",
    "# count_words(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "#!conda install -c anaconda beautifulsoup4 --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# resp = urlopen('https://stepik.org/media/attachments/lesson/245130/6.html') # скачиваем файл\n",
    "# html = resp.read().decode('utf8') # считываем содержимое\n",
    "# soup = BeautifulSoup(html, 'html.parser') # делаем суп\n",
    "# table = soup.find('table', attrs = {'class' : 'wikitable sortable'})\n",
    "# cnt = 0\n",
    "# for tr in soup.find_all('tr'):\n",
    "#     cnt += 1\n",
    "#     for td in tr.find_all(['td', 'th']):\n",
    "#         cnt *= 2\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests as re\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# response = re.get('https://stepik.org/media/attachments/lesson/209723/5.html').text\n",
    "# soup = BeautifulSoup(response, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_list = []\n",
    "\n",
    "# for tag in soup.find_all('td'):\n",
    "#     result_list.append(tag.text)\n",
    "\n",
    "# summation = 0\n",
    "# for value in result_list:\n",
    "#     summation += int(value)\n",
    "\n",
    "# summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda xlrd --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/xz7yq7t94yjghdts46wr7z580000gp/T/tmpl_voldgu\n",
      "<xlrd.book.Book object at 0x13dc04590>\n",
      "39650.2\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import xlrd\n",
    "# import urllib\n",
    "\n",
    "# link = 'https://stepik.org/media/attachments/lesson/245266/tab.xlsx'\n",
    "# file_name, headers = urllib.request.urlretrieve(link)\n",
    "# print (file_name)\n",
    "# wb = xlrd.open_workbook(file_name)\n",
    "# print(wb)\n",
    "\n",
    "# sheet_names = wb.sheet_names()\n",
    "# sh = wb.sheet_by_name(sheet_names[0])\n",
    "# nmin = sh.row_values(6)[2]\n",
    "# for rownum in range(7, 27):\n",
    "#     temp = sh.row_values(rownum)\n",
    "#     nmin = min(nmin, temp[2])\n",
    "# print(nmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
