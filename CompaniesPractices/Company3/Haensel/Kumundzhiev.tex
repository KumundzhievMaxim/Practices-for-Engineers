\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[latin2]{inputenc}
\usepackage[english]{babel}
\usepackage{palatino}
\usepackage{amsmath}   
\usepackage{amssymb}
\usepackage{mathtools}
\setlength{\parindent}{0pt}
\usepackage[left=2.5cm,right=2.5cm,top=4cm,bottom=4cm]{geometry}
\usepackage{stuki}
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
 
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\begin{document}
\maketitle
\begin{center}
		\author{\Large Maksim Kumundzheiv}
	\end{center}

Company: Haensel AMS\newline
Position: MACHINE LEARNING ENGINEER / DATA SCIENTIST\newline
This solution was submitted and prepared by Kumundzhiev Maksim for the Data Challenge. I declare that this solution is my own work. I have not copied or used third party solutions. I have not passed my solution to another candidates.
\bigskip

\newpage
\textbf{Data description}

(CSV file, sample.zip - 40 MB unzipped, 2 MB zipped) containing 66k records/rows and 295 features/columns. The target variable is the last column (the 296th) with values/classes A,B,C,D,E.\newline

\textbf{The task}\newline

1. Make an initial data analysis.
Visualize the main characteristics of the datset and try to highlight potential helpful structures in the data.

2. Fit some ML model(s).
Train and evaluate different models and please explain briefly your choices for the models and their pros & cons.

3. Show with some X-validation the power of your model and comment the results.
We are of course interested in the overall performance, but much more in the performance per class and especially in the under represented ones.

4. Present us the results and the steps you have taken.
If possible add also some critical thinking and next possible steps. But mainly explain why your results are good and what insights we can obtain from it.\newline

\textbf{Solution}\newline

\textbf{Defining the problem}\newline
Among all Machine Learning problems we can highlight the most basic:
\begin{itemize}
  \item Classification of an instance to one of the categories based on its features;
  \item Regression - prediction of a numerical target feature based on other features of an instance;
  \item Clustering - identifying partitions of instances based on the features of these instances so that the members within the groups are more similar to each other than those in the other groups;
  \item Anomaly detection - search for instances that are "greatly dissimilar" to the rest of the sample or to some group of instances;
  \item In our case it was the multiclass classification problem
\end{itemize}\newline

\newpage
\textbf{Part 1. Untuned solution:}\newline
\begin{itemize}
    \item Check the target - the number of classes - understand which type of task we have
    \subitem if numeric - okay;
    \subitem categorical - one hot encoding or dummy function;
    \item The target - check balanced / imbalanced situation + visualisation
    \item Check the types of features
    \subitem if numeric - okay;
    \subitem if categorical - one hot encoding or dummy function
    \item Check the distribution of data
    \subitem if distribution is NOT high(all the random values is near with mean) - okay
    \subitem if distribution is high - normalize / standardize
    \item Split data (lots of ways(Validation, CrossValidation))
    \item Straight solution with few models, define benchmark
\end{itemize}
\textbf{Part 2. Tuned solution:}\newline
\begin{itemize}
\item Implement Feature Selection and define features with the most Information Gain (reduce the number of features)
\item  Split data (lots of ways(Validation, CrossValidation))
\item  Straight solution with few models, define benchmark for modified data
\item Compare results
\end{itemize}

\newpage
\textbf{"Defining" the evaluating metrics}\newline
\begin{itemize}
  \item Accuracy - intuitive, obvious metric - the proportion of correct answers of the algorithm.\newline
  
  $accuracy = \frac{TP + TN}{TP + TN + FP + FN}$\newline
\end{itemize}

\textbf{EDA - Exploration Data Analysis}\newline
\begin{itemize}
\item Exploring the data, we could assume lots of 0 values - that looked like sparse matrix. In order to fix we are demanded to make more attentive analysis or to reduce the number of futures. (PCA, SVD, RFE, Borura etc)\newline
\item Also, it was possible to notice that the values in the data were very scattered. In order to fix we need to use normalization or standardization(In my case I used normalization). 
\item As well, the target value was imbalanced. In this case there were few ways: to use oversampling/undersampling techniques, to combine smaller classes with each other(combine smaller 4 classes out of 5) and get the Binary Classification task or try to go straight and make downscale benchmark. 
\item Likewise, after creating a correlation matrix(which is one of the most simple and effective statistical filters) it is very clearly visible that lots of variables are strongly correlated. To fix this, you can remove one value in a bunch of correlated objects or to use Feature Engineering approaches. 
\end{itemize}
\newpage
\textbf{Machine Learning approaches}\newline 

Due to the fact that we have the trivial problem of Multi Classification, I decided to try the standard algorithms of machine learning, such as:
\begin{itemize}
  \item - Logistic Regression ($sklearn.linear_model.LogisticRegression$);
  \item - SGD Classifier ($sklearn.linear_model.SGDClassifier$);
  \item - KNeighbors Classifier ($sklearn.neighbors.KNeighborsClassifier$);
  \item - RandomForest Classifier ($ sklearn.ensemble.RandomForestClassifier$);
  \item - GradientBoostingClassifier($sklearn.ensemble.GradientBoostingClassifier$);
  \item - Feature Engineering (Selection) - Boruta ($from boruta import BorutaPy$)
  \end{itemize}\newline
  Due to the condition that the test data was not given, it was necessary to validate on the train data. Here I used KFold cross validation approach. Also, in some algorithms I used a loop in order to find the best parameters of the model. Regarding the hyper parameters, I used well-known values, confirmed by prooved mathematics theorems and experience.\newline

\textbf{Feature Engineering (Selecting)}\newline 
I decided to use the \href{https://github.com/scikit-learn-contrib/boruta_py}{Boruta}\newline
Boruta is an all relevant feature selection method, while most other are minimal optimal; this means it tries to find all features carrying information usable for prediction, rather than finding a possibly compact subset of features on which some classifier has a minimal error.\newline
As a result I got 14 features with the highest Information Gain. \newline

\newpage
\textbf{Comparison of evaluation}\newline
\title{Algorithm's Evaluating}
\begin{center}
\begin{tabular}{ c c c }
       & Result \\ 
 $\stackrel{KNeighborsClassifier} $ & 0.6298\\  
 $\stackrel{RandomForest Classifier} $ & 0.7088\\   
 $\stackrel{Logistic regression} $ & 0.7085\\   
 $\stackrel{SGDClassifier} $ & 0.7090\\
 $\stackrel{GBM} $ & 0.9258 \\
 $\stackrel{KNeighborsClassifier + Boruta} $ & 0.6253\\  
 $\stackrel{RandomForest Classifier + Boruta} $ & 0.7088 \\
 $\stackrel{Logistic regression + Boruta} $ & 0.7085 \\   
 $\stackrel{SGDClassifier + Boruta} $ & 0.7090 \\
 $\stackrel{GBM + Boruta} $ & 0.9973  \\
 
\end{tabular}
\end{center}\newline

\textbf{The ways of improving the r insuresults}\newline it is correct;

\begin{itemize}
  \item - First of all I would like to test on new test data(develop synthetic data) and actually check on overfitting;
  \item - Also, try to decrease the time complexity for GBM or implement LightGBM etc;
  \item - As well, check another Feature Selector in order to insure the selected features are correct;
  \item - Re-sampling Dataset (SMOTE) etc;
  \end{itemize}\newline
\end{document}