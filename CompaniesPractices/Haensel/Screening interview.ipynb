{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About yourself**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am 23 years old. I graduated from the Kuban State University with a degree in Computer Science. \n",
    "Now I am studying for a master's degree in Computer Science on the specialty of Data Science. \n",
    "I am also the holder of a grant from the European University of Innovation and Technology(EIT) which is located at \n",
    "Budapest. \n",
    "I received this grant for developing an application for indoor navigation in all buildings of my university. \n",
    "Also, I am a member of the DSSG society in Berlin and actively participate as a volunteer at various events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VerticalScale and WHAT WE DO NOWADAYS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 4 months now I have been working at VerticalScale as a Scientist Date. \n",
    "We have 7 developers in our team, 4 of whom are involved in Web Development and 3 others, \n",
    "including me developing Machine Learning algorithms for solving such problems as: \n",
    "Fraud Prediction, Risk Management, Factoring Scoring, Algorithmic Trading Process, Human Analysis, \n",
    "Employee Attrition prediction.\n",
    "Today, most of the time we spend on the building of development processes,\n",
    "the reason is that we still expect data from customers. \n",
    "So, regarding the data for now, we collect data from open sources, \n",
    "use various api, try to deploy a full-fledged database for our data, \n",
    "and of course we work with data and try to find interesting insights in them and \n",
    "fit different models and understand which ones will be better.\n",
    "\n",
    "Models which we usually use: \n",
    "GBM, LightGBM, XGBoost and usual models as Logistic Regression, SVM and another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions regarding Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithms: <br>\n",
    "\n",
    "<b>Bias</b> is the simplifying assumptions made by the model to make the target function easier to approximate.<br> \n",
    "<b>Variance</b> is the amount that the estimate of the target function will change given different training data.<br> \n",
    "Trade-off is tension between the error introduced by the bias and the variance.\n",
    "\n",
    "<b>- KNN</b>\n",
    "The task of classification in machine learning is the task of assigning an object to one of the predefined classes<br> on  the basis of its formalized features. Each of the objects in this problem is represented as a vector in<br> \n",
    "N-dimensional space, each dimension in which is a description of one of the features of the object.<br>\n",
    "Parrams - number of neighbours, metrics for distance<br>\n",
    "\n",
    "<b> - Logistic Regression</b>\n",
    "The basic idea of a linear classifier is that the feature space can be divided by the hyperplane into two <br>\n",
    "half-spaces, in each of which one of the two values of the target class is predicted.<br>\n",
    "If this can be done without errors, then the training set is called linearly separable.<br>\n",
    "\n",
    "<b> - SGDClassifier</b>\n",
    " The objective of the support vector machine algorithm is to find the hyperplane that has the maximum margin in an<br> N-dimensional space(N — the number of features) that distinctly classifies the data points.<br>\n",
    "SGD is an Stochastic Gradient Descent-based (this is a general optimization method!) optimizer which can optimize <br> many different convex-optimization problems (actually: this is more or less the same method used in all those <br>Deep-Learning approaches; so people use it in the non-convex setting too; throwing away theoretical-guarantees).\n",
    "\n",
    "<b> - SVM</b>\n",
    "SVM is a support-vector machine which is a special linear-model. From a theoretical view it's a <br>\n",
    "convex-optimization problem and we can get the global-optimum in polynomial-time. There are many different<br> optimization-approaches.\n",
    "\n",
    "<b> - RandomForestClassifier</b>\n",
    " A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples<br>\n",
    " of the dataset and uses averaging to improve the predictive accuracy and control over-fitting<br>\n",
    "\n",
    "**ID3, C4.5, CART, CHAID (to build a Decision Trees)**<br>\n",
    "**Gini impurity, Information gain, Entropy (Branching criteria)<br>**\n",
    "\n",
    "<b>Pruning</b> is a technique in machine learning and search algorithms that reduces the size of decision <br>\n",
    "trees by removing sections of the tree that provide little power to classify instances. Pruning reduces the<br> complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.<br>\n",
    "\n",
    "<b>Bootstrap aggregating, also called bagging</b>, is a machine learning ensemble meta-algorithm designed<br> \n",
    "to improve the stability and accuracy of machine learning algorithms used in statistical classification and<br>\n",
    "regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to <br>\n",
    "decision tree methods, it can be used with any type of method. Bagging is a special case of the model <br>\n",
    "averaging approach.\n",
    "\n",
    "<b> - GradientBoostingClassifier</b>\n",
    " GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary<br> differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient<br>\n",
    "of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single<br> regression tree is induced.<br>\n",
    " ---learning rate shrinks the contribution of each tree by learning_rate\n",
    " \n",
    " - FOR FEATURE ENGINEERING(SELECTION) - BORUTA\n",
    " Heuristic algorithm for the selection of significant features based on the use of Random Forest [5]. <br>\n",
    " The essence of the algorithm is that at each iteration features that have a Z-measure less than<br>\n",
    " the maximum Z-measure among the added features are removed. To get the Z-measure of a trait, it is necessary<br>\n",
    " to calculate the importance of the trait obtained using the built-in algorithm in the Random Forest, and<br>\n",
    "divide it by the standard deviation of the importance of the trait. Added features<br>\n",
    "are obtained as follows: the signs that are available in the sample are copied, and then each new sign is <br>\n",
    "filled by shuffling its values. In order to obtaining statistically significant results, this procedure is<br>\n",
    "repeated several times, variables are generated independently at each iteration.\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**26) What is a cost function?**\n",
    "(Text Answer) A cost function is a measure of the accuracy of the neural network with respect to given training sample and expected output. It is a single value, nonvector as it gives the performance of the neural network as a whole. It can be calculated as below Mean Squared Error function:- MSE=1n∑i=0n(Y^i–Yi)^2. Where Y^ and desired value Y is what we want to minimize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**27) What is a backpropagation?**\n",
    "(Text Answer) Backpropagation is training algorithm used for multilayer neural network. In this method, we move the error from an end of the network to all weights inside the network and thus allowing efficient computation of the gradient. It can be divided into several steps as follows:\n",
    "• Forward propagation of training data in order to generate output.\n",
    "• Then using target value and output value error derivative can be computed with respect to\n",
    "output activation.\n",
    "• Then we back propagate for computing derivative of error with respect to output activation on previous and continue this for all the hidden layers.\n",
    "• Using previously calculated derivatives for output and all hidden layers we calculate error derivatives with respect to weights. And then we update the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**28) What is weight initialization in neural networks?**\n",
    "(Text Answer) Weight initialization is one of the very important steps. A bad weight initialization can prevent a network from learning but good weight initialization helps in giving a quicker convergence and a better overall error. Biases can be generally initialized to zero. The rule for setting the weights is to be close to zero without being too small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**29) What is the role of the activation function? List the activation functions which you know.**\n",
    "(Text Answer) The activation function is used to introduce non-linearity into the neural network helping it to learn more complex function. Without which the neural network would be only able to learn linear function which is a linear combination of its input data.\n",
    "  ACTIVATION FUNCTIONS\n",
    "• Step function activation\n",
    "• Linear activation function\n",
    "• Sigmoid\n",
    "• Hyperbolic tangent\n",
    "• ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**34) List evaluation metrics which you usually use for Classification and Regression task.**\n",
    "\n",
    "<b>Classification:</b>\n",
    "• Classification Accuracy.\n",
    "• Logarithmic Loss.\n",
    "• Area Under ROC Curve.\n",
    "• Confusion Matrix.(F1, Precision, Recall)<br>\n",
    "     \n",
    "• Classification Report. \n",
    "\n",
    "<b>Regression:</b>\n",
    "• Mean Absolute Error.\n",
    "• Mean Squared Error.\n",
    "• R^2.\n",
    "• RMSE (Root Mean Square Error)\n",
    "• BLEU (Bilingual Evaluation Understudy)(FOR NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression, LASSO.**\n",
    "Regularization methods(Ridge and LASSO) is a kind of penalty for the excessive complexity of the model, which allows you to protect yourself from overtraining if there are garbage features among them. You should not think that regularization happens only in linear models, and for boosting and for neural networks there are own regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
