{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras - Functional API.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nf6E888AgP3l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras Functional API\n",
        "This notebook shows how to use the  Functional API of Keras by implementing a 20 layer resnet. \n",
        "\n",
        "Up until this point we have only seen examples where layers were following each other sequentially. The input of the current layer was always the output of the previous layer. This setup was the standard until about 2014-15 when the InceptionNet and ResNet architectures appeared. Both have more complicated connections then simple sequential ones.\n",
        "\n",
        "Previous examples used the Sequential model API designed specifically for sequential models. The functional API let's you connect layers arbitrarily, in whatever way you want. It is very useful for more advanced architectures. Addtionally, it enables the network to have multiple inputs and outputs.\n",
        "\n",
        "## Functional API example\n",
        "First we will show how can the two layer network from the second lesson be written using Functional API. The network written in with the Sequential API looks like the following:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ijfAYgXp8odW",
        "colab_type": "code",
        "outputId": "05870569-26ae-43ba-f1e6-eabbbe8e5419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "seq_model = Sequential()\n",
        "seq_model.add(Dense(64, input_shape=(16,)))\n",
        "seq_model.add(Dense(64))\n",
        "seq_model.add(Dense(10))\n",
        "\n",
        "seq_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 64)                1088      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 5,898\n",
            "Trainable params: 5,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2UZhBZKN9av1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The same model with Functional API:"
      ]
    },
    {
      "metadata": {
        "id": "eu3NQJW27xGj",
        "colab_type": "code",
        "outputId": "5b753a14-c242-418b-a400-fc91c2fed8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# This returns a tensor\n",
        "input = Input(shape=(16,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "# The parameters are the inputs, and the result of the call is the output tensor.\n",
        "output1 = Dense(64, activation='relu')(input)\n",
        "output2 = Dense(64, activation='relu')(output1)  # Here we connect the output of the first layer to the second layer\n",
        "predictions = Dense(10, activation='softmax')(output2)  # Here we connect the output of the second layer to the last layer\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "func_model = Model(inputs=input, outputs=predictions)\n",
        "func_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                1088      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 5,898\n",
            "Trainable params: 5,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EhhkJa1T7wOa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Basically, in the Functional API you have to declare how the layers are connected by passing the outputs of a layer as an input to another layer. Each layer is a Python callable and it expects as input a placeholder tensor and produces another tensor. The shape of the Input tensor has to be specified, from then on shapes are automatically calculated.\n"
      ]
    },
    {
      "metadata": {
        "id": "ep6rXoUSgP4a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The ResNet architecture\n",
        "\n",
        "The next example contains the newer variant of ResNet trained on CIFAR10 dataset. The architecture can be viewed [here](http://vegesm.web.elte.hu/resnet-20-keras.png).\n",
        "\n",
        "\n",
        "### Setup\n",
        "First do some imports needed later:"
      ]
    },
    {
      "metadata": {
        "id": "0CpPfeOggP4e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UmwN68sRgP4o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, load and normalize the data:"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "tOPG4DjjgP4p",
        "colab_type": "code",
        "outputId": "2db35bbf-83d8-4db8-f0c7-1420447cc300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Normalize data by subtracting the mean, slightly improves the results\n",
        "x_train_mean = np.mean(x_train, axis=0)\n",
        "x_train -= x_train_mean\n",
        "x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XYCQDDHbgP5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating the nework\n",
        "The `resnet_layer` definition is the 2D Convolution-Batch Normalization-Activation stack builder which has a few input parameters:\n",
        "- inputs (tensor): input tensor from input image or previous layer\n",
        "- num_filters (int): Conv2D number of filters\n",
        "- kernel_size (int): Conv2D square kernel size\n",
        "- strides (int): Conv2D  stride dimensions\n",
        "- activation (string): activation name\n",
        "- batch_normalization (bool): whether to include batch normalization\n",
        "- conv_first (bool): conv-bn-activation (True) or\n",
        "- bn-activation-conv (False)\n",
        "\n",
        "and it returns with an `x (tensor)` which is a tensor as input to the next layer. \n",
        "(tensor): tensor as input to the next layer."
      ]
    },
    {
      "metadata": {
        "id": "gAz4x72EgP5F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ldkXB1wlgP5n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The ResNet presented here have 3 stages, each stage having two residual blocks. One residual block contains a stack of (1 x 1)-(3 x 3)-(1 x 1) convolutions with BatchNormalization and ReLU layers. BN-ReLU-Conv2D or also known as bottleneck layer.\n",
        "\n",
        "At the beginning of each stage, the feature map size is halved (downsampled) by the end of the first residual block, while the number of filter maps is doubled. After that, within each stage, the reisudal blocks' input and output size does not change.\n",
        "\n",
        "Features maps sizes and number of filters after each stage:\n",
        "- conv1 (pre stage 0): 32x32,  16\n",
        "- stage 0: 32x32,  64\n",
        "- stage 1: 16x16, 128\n",
        "- stage 2:  8x8,  256"
      ]
    },
    {
      "metadata": {
        "id": "V3MWM_lygP5q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resnet_v2(input_shape, num_res_blocks, num_classes=10):\n",
        "    \"\"\"\n",
        "    Creates a ResNetv2 model.\n",
        "    \n",
        "    Parameters:\n",
        "      input_shape: shape of input image tensor\n",
        "      num_res_blocks: number of residual blocks per stages\n",
        "      num_classes: number of output classes (CIFAR10 has 10)\n",
        "    Returns:\n",
        "      The Keras model.\n",
        "    \"\"\"\n",
        "        \n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    # ResNet first performs a Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # Bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            \n",
        "            # In each stage the first block decreases the size of the feature maps \n",
        "            # and increases the number of filters. We can only add tensors of the same dimensions,\n",
        "            # so in the skip-connection we have to resize the input.\n",
        "            # We do that by simply doing a 1x1 convolution\n",
        "            if res_block == 0:\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    x = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(x)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6POWG5mxgP5y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we train our neural network. The only new thing here is the callback property. Callbacks can be used in Keras to add extra steps to training that are performed on every epoch/iteration. Some examples:\n",
        "- `ModelCheckpoint`: saves the model on every epoch. Useful if you have a large model that takes a long time to train and you want to save the model every now and then in case the computer crashes.\n",
        "- `CsvLogger`: saves the train/validation accuracy and loss during training, useful for visualisations.\n",
        "- `ReduceLROnPlateau`: decreases the learning rate if the validation loss has stopped decreasing.\n",
        "\n",
        "We use the `ReduceLROnPlateau` callback that divides the learning rate by $\\sqrt{10}$ if the validation loss did not decrease for five epochs. It is an often useful startegy to decrease the learning rate over time.\n"
      ]
    },
    {
      "metadata": {
        "id": "VYnMJwYdgP51",
        "colab_type": "code",
        "outputId": "2577d9ec-078c-4b9f-e59d-58d56b30b149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        }
      },
      "cell_type": "code",
      "source": [
        "model = resnet_v2(input_shape=input_shape, num_res_blocks=2)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=1e-3),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# This\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [lr_reducer]\n",
        "\n",
        "# Run training\n",
        "# Training ResNet is slower than previous network, so it is trained only for 10 epochs.\n",
        "# To train it fully, you can run it for 100 or 200 epochs, if you have time!\n",
        "model.fit(x_train, y_train, batch_size=32,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test),\n",
        "          shuffle=True,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 175s 4ms/step - loss: 1.6880 - acc: 0.5174 - val_loss: 1.4072 - val_acc: 0.6016\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 168s 3ms/step - loss: 1.2400 - acc: 0.6606 - val_loss: 1.7359 - val_acc: 0.4613\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 165s 3ms/step - loss: 1.0586 - acc: 0.7203 - val_loss: 1.2764 - val_acc: 0.6509\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 165s 3ms/step - loss: 0.9461 - acc: 0.7560 - val_loss: 1.1159 - val_acc: 0.7067\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 165s 3ms/step - loss: 0.8731 - acc: 0.7857 - val_loss: 1.0746 - val_acc: 0.7205\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 166s 3ms/step - loss: 0.8135 - acc: 0.8031 - val_loss: 1.1103 - val_acc: 0.7156\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 165s 3ms/step - loss: 0.7688 - acc: 0.8198 - val_loss: 1.1191 - val_acc: 0.7222\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 163s 3ms/step - loss: 0.7358 - acc: 0.8332 - val_loss: 0.9177 - val_acc: 0.7780\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 160s 3ms/step - loss: 0.7064 - acc: 0.8430 - val_loss: 1.0886 - val_acc: 0.7271\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 159s 3ms/step - loss: 0.6782 - acc: 0.8553 - val_loss: 1.1078 - val_acc: 0.7317\n",
            "10000/10000 [==============================] - 5s 476us/step\n",
            "Test loss: 1.1077691575050355\n",
            "Test accuracy: 0.7317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x4s5efjAJ81N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have achieved 73% accuracy on CIFAR-10. With training the network much longer (200 epochs), one can achieve 91-92% accuracy."
      ]
    }
  ]
}